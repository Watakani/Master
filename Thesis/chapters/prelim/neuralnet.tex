\begin{theorem}[\cite{nn_uni}]\label{thm:uni_nn}
    Let \(\mathcal{X}\subset \R^D\) be compact and \(\mathcal{NN}^{\phi}_{n,m}\)
    be a neural network with \(n\) inputs, \(m\) outputs, and \(\phi\colon \R \rightarrow \R\) 
    be an activation function which are an nonaffine continuous function with at least one point
    being continuously differentiable with nonzero derivative. Then, if the hidden layers
    contain \(n+m+2\) and arbitrary depth, the network is dense in \(C(\mathcal{X}; \R^D)\).
\end{theorem}
