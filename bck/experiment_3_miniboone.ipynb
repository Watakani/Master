{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NormalizingFlows.src.train import train_backward\n",
    "from NormalizingFlows.src.scores import log_likelihood\n",
    "from NormalizingFlows.src.utils import update_device, load_best_model, load_checkpoint_model\n",
    "\n",
    "from NormalizingFlows.src.flows import *\n",
    "from NormalizingFlows.src.data.density.miniboone import Miniboone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_visible_devices(*devices: int) -> None:\n",
    "    '''Utility to set visible Cuda devices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    devices : List[int]\n",
    "        Index of cuda devices to make available for use.\n",
    "    '''\n",
    "    assert all([d >= 0 for d in devices]), f\"Not all devices are CUDA devices!\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join([str(i) for i in devices])\n",
    "    \n",
    "\n",
    "def set_devices(*devices: int):\n",
    "    '''Utility to set Cuda device(s).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    devices : List[int]\n",
    "        Index of cuda devices to make available for use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.device or List[torch.device] of currently available CUDA devices.\n",
    "    '''\n",
    "    assert len(devices) > 0, f'Device list is empty, no devices set.'\n",
    "    if len(devices) == 1:\n",
    "        if devices[0] >= 0:\n",
    "            set_visible_devices(devices[0])\n",
    "            return torch.device(0)\n",
    "        else:\n",
    "            return torch.device('cpu')\n",
    "\n",
    "    else:\n",
    "        set_visible_devices(*devices)\n",
    "        return [torch.device(i) for i in range(len(devices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = set_devices(5) #torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Miniboone() \n",
    "dim_input = dataset.dim_input\n",
    "num_trans = 10 #Must be even\n",
    "\n",
    "dim_hidden = [512,512]\n",
    "flows = {}\n",
    "flow_forward = False\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 32\n",
    "num_exp = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_a = lambda x: torch.abs(torch.arcsinh(x))\n",
    "\n",
    "two = torch.tensor(2)\n",
    "def softsaturate(a):\n",
    "    neg = 2 * (a < 0) * F.softplus(a)\n",
    "    pos = (a >= 0) * (torch.arcsinh(a) + 2 * torch.log(two))\n",
    "    return neg + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ContinuousPiecewiseAffineAffine\n",
    "a_param = f_a\n",
    "c_param = torch.sigmoid\n",
    "\n",
    "name = 'ContinuousPiecewiseAffineAffine exp3 min'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_affinecontinuous_trans(num_trans, flow_forward, a_param=a_param, c_param=c_param)\n",
    "    affconpiec_coup_alt = create_flows_with_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [affconpiec_coup_alt]\n",
    "    flows[name][-1].name = f'{name} {i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alt. Lin. Aff.Con\n",
    "a_param = f_a\n",
    "c_param = softsaturate\n",
    "\n",
    "name = 'Alternating Linear_AffineContinuous exp3 min'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_alt_linear_affinecontinuous_trans(num_trans, dim_input, flow_forward, a_param=a_param, c_param=c_param)\n",
    "    linaffcont_coup_rand = create_flows_with_alt_identity_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [linaffcont_coup_rand]\n",
    "    flows[name][-1].name = f'{name} {i}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alt. Lin. Aff.\n",
    "a_param = softsaturate\n",
    "\n",
    "name = 'Alternating Linear_Affine exp3 min'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_alt_linear_affine_trans(num_trans, dim_input, flow_forward,a_param=a_param)\n",
    "    linaff_coup_rand = create_flows_with_alt_identity_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [linaff_coup_rand]\n",
    "    flows[name][-1].name = f'{name} {i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousPiecewiseAffineAffine exp3 min 1 Epoch: 0 Batch number: 150   4020.72803\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (32, 43)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([43]), covariance_matrix: torch.Size([43, 43])), but found invalid values:\ntensor([[-0.3487,  0.0993,  0.1404,  ..., -0.1143, -0.0712, -0.1358],\n        [-0.1914,  0.4113,  0.3118,  ..., -0.0384,  0.1890,  0.1152],\n        [-0.2728,  0.2533,  0.1825,  ..., -0.0027,  0.1163, -0.0139],\n        ...,\n        [-0.3608,  0.0125,  0.1007,  ..., -0.1167, -0.1711, -0.2473],\n        [-0.1066,  0.9703,  0.5091,  ...,  0.0660,  0.5353,  0.3200],\n        [-0.3494,  0.0913,  0.1469,  ..., -0.1143, -0.0801, -0.1086]],\n       device='cuda:0', grad_fn=<IndexBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3292434/2323870695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Train and append losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         losses[flowname].append(\n\u001b[0;32m---> 16\u001b[0;31m             train_backward(\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/train.py\u001b[0m in \u001b[0;36mtrain_backward\u001b[0;34m(model, train_data, optimizer, epochs, batch_size, print_n, save_checkpoint, save_best, burn_in, loss_func)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_forward\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nf.py\u001b[0m in \u001b[0;36mbackward_flow\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/basedistr.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;34m\"Expected value argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (32, 43)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([43]), covariance_matrix: torch.Size([43, 43])), but found invalid values:\ntensor([[-0.3487,  0.0993,  0.1404,  ..., -0.1143, -0.0712, -0.1358],\n        [-0.1914,  0.4113,  0.3118,  ..., -0.0384,  0.1890,  0.1152],\n        [-0.2728,  0.2533,  0.1825,  ..., -0.0027,  0.1163, -0.0139],\n        ...,\n        [-0.3608,  0.0125,  0.1007,  ..., -0.1167, -0.1711, -0.2473],\n        [-0.1066,  0.9703,  0.5091,  ...,  0.0660,  0.5353,  0.3200],\n        [-0.3494,  0.0913,  0.1469,  ..., -0.1143, -0.0801, -0.1086]],\n       device='cuda:0', grad_fn=<IndexBackward0>)"
     ]
    }
   ],
   "source": [
    "losses = {n: [] for n in flows.keys()}\n",
    "optimizers = {n: [] for n in flows.keys()}\n",
    "\n",
    "for flowname in flows.keys():\n",
    "    for j in range(num_exp):\n",
    "        # Initialize flow\n",
    "        flow = flows[flowname][j]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-4, weight_decay=0)\n",
    "        optimizers[flowname].append(optimizer)\n",
    "        \n",
    "        # Train and append losses\n",
    "        losses[flowname].append(\n",
    "            train_backward(\n",
    "                flow, \n",
    "                dataset.get_training_data(), \n",
    "                optimizer, \n",
    "                epochs, \n",
    "                batch_size, \n",
    "                print_n=10, \n",
    "                save_checkpoint=True, \n",
    "                burn_in=-1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Move flow to CPU\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_flows = {n:[] for n in flows.keys()}\n",
    "for flowname in flows.keys():\n",
    "    for i in range(num_exp):\n",
    "        best_flows[flowname].append(load_best_model(flows[flowname][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scale = True\n",
    "from_iter = 50 * batch_size\n",
    "\n",
    "fig, ax = plt.subplots(2, len(flows), figsize=(12,6))\n",
    "for j, flowname in enumerate(flows.keys()):\n",
    "    for i in range(num_exp):\n",
    "        ax[0,j].plot(losses[flowname][i], label=f'exp{i}', alpha=0.8)\n",
    "        ax[0,j].set_title(f'{flowname}')\n",
    "        if log_scale:\n",
    "            ax[0, j].set_yscale('log')\n",
    "        ax[0, j].legend()\n",
    "\n",
    "log_scale = True\n",
    "for j, flowname in enumerate(flows.keys()):\n",
    "    for i in range(num_exp):\n",
    "        x = np.arange(len(losses[flowname][i]))\n",
    "        ax[1,j].plot(x[from_iter:], losses[flowname][i][from_iter:], label=f'exp{i}', alpha=0.8)\n",
    "        if log_scale:\n",
    "            ax[1, j].set_yscale('log')\n",
    "        ax[1, j].legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_lh_results = {\n",
    "    'train': {n:[] for n in flows}, \n",
    "    'test': {n:[] for n in flows}, \n",
    "    'sample': {n:[] for n in flows},\n",
    "}\n",
    "cur_res = 'train'\n",
    "\n",
    "print('Results based on training data:' + '\\n')\n",
    "\n",
    "# Function for CI\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    import scipy.stats\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    " \n",
    "\n",
    "train_data = dataset.get_training_data()\n",
    "mean_target = torch.mean(dataset.evaluate(train_data.to(device))).detach().numpy()\n",
    "for flow in best_flows:\n",
    "    for i in range(num_exp):\n",
    "        log_lik, mean = log_likelihood(train_data, flows[flow][i])\n",
    "        flow_lh_results[cur_res][flow].append(mean[0])\n",
    "        print(\"Mean loglikelihood for {}: {}\".format(f'{flow} {i}', mean))\n",
    "        print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "        print()\n",
    "    \n",
    "    m, h = mean_confidence_interval(flow_lh_results[cur_res][flow])\n",
    "    print('-'*75)\n",
    "    print(f'{flow} -- Mean LogLH Bootstrap: {m:7.4f} ± {h:7.4f}')\n",
    "    print('-'*75)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on test data' + '\\n')\n",
    "cur_res = 'test' \n",
    "\n",
    "test_data = dataset.get_test_data()\n",
    "mean_target = torch.mean(dataset.evaluate(test_data)).detach().numpy()\n",
    "for flow in best_flows:\n",
    "    for i in range(num_exp):\n",
    "        log_lik, mean = log_likelihood(test_data, flows[flow][i])\n",
    "        flow_lh_results[cur_res][flow].append(mean[0])\n",
    "        print(\"Mean loglikelihood for {}: {}\".format(f'{flow} {i}', mean))\n",
    "        print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "        print()\n",
    "    \n",
    "    m, h = mean_confidence_interval(flow_lh_results[cur_res][flow])\n",
    "    print('-'*75)\n",
    "    print(f'{flow} -- Mean LogLH Bootstrap: {m:7.4f} ± {h:7.4f}')\n",
    "    print('-'*75)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))\n",
    "\n",
    "# for flow in best_flows:\n",
    "#     log_lik, mean = log_likelihood(test_data, flow)\n",
    "#     print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "#     print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "    \n",
    "\n",
    "# print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on new sample from each flow:' + '\\n')\n",
    "cur_res = 'sample'\n",
    "\n",
    "for flow in best_flows:\n",
    "    for i in range(num_exp):\n",
    "        with torch.no_grad():\n",
    "            sample, log_prob = flows[flow][i].sample(800)\n",
    "            sample_last = sample[-1]\n",
    "\n",
    "        mean_target = torch.mean(dataset.evaluate(sample_last)).detach().numpy()\n",
    "        log_lik, mean = log_likelihood(sample_last, flows[flow][i])\n",
    "        flow_lh_results[cur_res][flow].append(mean[0])\n",
    "        print(\"Mean loglikelihood for {}: {}\".format(f'{flow} {i}', mean))\n",
    "        print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "        print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))   \n",
    "        print()\n",
    "    \n",
    "    m, h = mean_confidence_interval(flow_lh_results[cur_res][flow])\n",
    "    print('-'*75)\n",
    "    print(f'{flow} -- Mean LogLH Bootstrap: {m:7.4f} ± {h:7.4f}')\n",
    "    print('-'*75)\n",
    "    print()\n",
    "\n",
    "\n",
    "# for flow in best_flows:\n",
    "#     with torch.no_grad():\n",
    "#         sample, log_prob = flow.sample(800)\n",
    "#         sample_last = sample[-1]\n",
    "\n",
    "#     #sample_last.clip_(-7, 7)\n",
    "#     mean_target = torch.mean(dataset.evaluate(sample_last)).detach().numpy()\n",
    "#     log_lik, mean = log_likelihood(sample_last, flow)\n",
    "\n",
    "#     print('Check for NANs: ', torch.isnan(sample_last).any().item())\n",
    "#     print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "#     print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))    \n",
    "#     print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
