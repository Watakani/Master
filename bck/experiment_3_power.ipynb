{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from NormalizingFlows.src.train import train_backward\n",
    "from NormalizingFlows.src.scores import log_likelihood\n",
    "from NormalizingFlows.src.utils import update_device, load_best_model, load_checkpoint_model\n",
    "\n",
    "from NormalizingFlows.src.flows import *\n",
    "from NormalizingFlows.src.data.density.power import Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_visible_devices(*devices: int) -> None:\n",
    "    '''Utility to set visible Cuda devices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    devices : List[int]\n",
    "        Index of cuda devices to make available for use.\n",
    "    '''\n",
    "    assert all([d >= 0 for d in devices]), f\"Not all devices are CUDA devices!\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join([str(i) for i in devices])\n",
    "    \n",
    "\n",
    "def set_devices(*devices: int):\n",
    "    '''Utility to set Cuda device(s).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    devices : List[int]\n",
    "        Index of cuda devices to make available for use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.device or List[torch.device] of currently available CUDA devices.\n",
    "    '''\n",
    "    assert len(devices) > 0, f'Device list is empty, no devices set.'\n",
    "    if len(devices) == 1:\n",
    "        if devices[0] >= 0:\n",
    "            set_visible_devices(devices[0])\n",
    "            return torch.device(0)\n",
    "        else:\n",
    "            return torch.device('cpu')\n",
    "\n",
    "    else:\n",
    "        set_visible_devices(*devices)\n",
    "        return [torch.device(i) for i in range(len(devices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = set_devices(6) #torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Power()\n",
    "dim_input = dataset.dim_input\n",
    "num_trans = 10 #Must be even\n",
    "\n",
    "dim_hidden = [512,512]\n",
    "flows = {}\n",
    "flow_forward = False\n",
    "\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 128\n",
    "num_exp = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_a = lambda x: torch.abs(torch.arcsinh(x))\n",
    "\n",
    "two = torch.tensor(2)\n",
    "def softsaturate(a):\n",
    "    neg = 2 * (a < 0) * F.softplus(a)\n",
    "    pos = (a >= 0) * (torch.arcsinh(a) + 2 * torch.log(two))\n",
    "    return neg + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Mean_field\n",
    "\n",
    "# name = 'Mean field exp3 pow' \n",
    "# flows[name] = []\n",
    "# for i in range(num_exp):\n",
    "#     transformations = create_constant_trans(num_trans, dim_input, flow_forward, a_param=F.softplus)\n",
    "#     mean_field = create_flows_with_identity(dim_input, transformations, flow_forward) \n",
    "#     flows[name] += [mean_field]\n",
    "#     flows[name][-1].name = f'{name} {i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Affine\n",
    "# a_param = softsaturate\n",
    "\n",
    "# name = 'Affine exp3 pow'\n",
    "# flows[name] = []\n",
    "# for i in range(num_exp):\n",
    "#     transformations = create_affine_trans(num_trans, flow_forward, a_param = a_param)\n",
    "#     aff_ar_alt = create_flows_with_IAR(dim_input, dim_hidden, transformations, 'alternate', flow_forward )\n",
    "#     flows[name] += [aff_ar_alt]\n",
    "#     flows[name][-1].name = f'{name} {i}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ContinuousPiecewiseAffineAffine\n",
    "a_param = f_a\n",
    "c_param = torch.sigmoid\n",
    "\n",
    "name = 'ContinuousPiecewiseAffineAffine exp3 pow'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_affinecontinuous_trans(num_trans, flow_forward, a_param=a_param, c_param=c_param)\n",
    "    affconpiec_coup_alt = create_flows_with_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [affconpiec_coup_alt]\n",
    "    flows[name][-1].name = f'{name} {i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alt. Lin. Aff.Con\n",
    "a_param = f_a\n",
    "c_param = torch.sigmoid\n",
    "\n",
    "name = 'Alternating Linear_AffineContinuous exp3 pow'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_alt_linear_affinecontinuous_trans(num_trans, dim_input, flow_forward, a_param=a_param, c_param=c_param)\n",
    "    linaffcont_coup_rand = create_flows_with_alt_identity_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [linaffcont_coup_rand]\n",
    "    flows[name][-1].name = f'{name} {i}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alt. Lin. Aff.\n",
    "a_param = torch.sigmoid\n",
    "\n",
    "name = 'Alternating Linear_Affine exp3 pow'\n",
    "flows[name] = []\n",
    "for i in range(num_exp):\n",
    "    transformations = create_alt_linear_affine_trans(num_trans, dim_input, flow_forward,a_param=a_param)\n",
    "    linaff_coup_rand = create_flows_with_alt_identity_AR(dim_input, dim_hidden, transformations, 'alternate', flow_forward)\n",
    "    flows[name] += [linaff_coup_rand]\n",
    "    flows[name][-1].name = f'{name} {i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousPiecewiseAffineAffine exp3 pow 1 Epoch: 3 Batch number: 300      0.01183\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (128, 6)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([6]), covariance_matrix: torch.Size([6, 6])), but found invalid values:\ntensor([[-0.2675,  1.4909, -0.3712, -0.9898, -0.3368, -0.2115],\n        [ 0.1577,  1.2616, -1.3436, -0.3440,  1.4938,  1.7304],\n        [ 1.2705, -0.7253, -1.3834,  1.0955,  0.1474, -2.7226],\n        [ 0.1669, -1.5974, -0.0104,  0.4517, -1.0284,  0.4189],\n        [ 0.1439,  0.1037,  0.0651, -0.4938,  1.4417,  0.2292],\n        [ 1.2223,  1.2371,  0.0612,  1.1641,  0.8810,  0.2647],\n        [-0.9399, -0.9608,  0.0506, -1.4231,  2.6052,  0.9567],\n        [ 0.7865,  0.8883, -0.2133,  1.3404, -0.0694,  0.3940],\n        [ 0.4920,  1.0669, -1.6345,  0.9416, -0.4395,  0.8280],\n        [-0.9693, -0.7449, -1.9134,  1.1051,  2.3863, -0.4489],\n        [-0.2993, -0.1012, -0.2123,  0.6692, -0.0909,  0.5488],\n        [-1.0780,  2.1041,  0.1545, -0.9265, -0.6986,  0.9396],\n        [ 0.1825, -0.1149,  1.3632,  0.6463, -0.1405, -0.3392],\n        [-0.5964, -1.1183, -0.6785, -1.0986, -0.5350,  0.7550],\n        [-0.3273, -0.0113,  0.9487,  0.7046, -0.1405,  0.8415],\n        [-0.5522, -0.4056, -1.7827,  0.1657,  0.8674, -0.5185],\n        [ 1.0819,  1.0422, -1.7015, -0.4206, -1.7066,  1.1457],\n        [-0.6300, -0.0191, -0.1210, -0.9106, -0.4360,  0.5464],\n        [-1.8629, -0.8358, -0.9142,  0.8180, -0.3310, -1.2395],\n        [ 0.5144,  0.4866,  1.2963,  0.4529,  0.3310,  0.3804],\n        [ 1.0431,  0.1718, -1.2663, -0.2353, -2.8000, -0.9505],\n        [-0.6831, -0.5029, -0.0101,  1.5395, -0.3148, -1.7299],\n        [-1.2932, -0.2658,  0.0957,  1.5588, -0.5864, -0.3406],\n        [-0.1132, -0.7710,  2.1153,  0.4383, -1.0933, -0.6227],\n        [-1.3670, -1.7658, -0.4166,  1.4362,  0.3822,  0.3269],\n        [-0.3028, -0.7639,  1.5479, -0.7053, -1.1363, -1.2451],\n        [-1.3119, -0.6996,  0.6513, -0.7777,  0.3136, -0.5138],\n        [-0.2780, -0.5162,  0.1677, -0.4806,  0.1984,  0.0408],\n        [ 0.0324,  0.2460, -0.6014,  0.2068,  0.5608,  2.9982],\n        [-0.3680,  0.9058,  0.3947, -1.4375, -0.8677,  0.2571],\n        [ 0.6253, -1.0956, -0.8828,  0.7321, -0.2714, -1.2713],\n        [ 0.5201, -1.9279,  1.0629,  0.1377, -0.8069, -1.1453],\n        [ 0.1128,  0.6105,  0.7916, -1.0500, -1.6494, -0.8789],\n        [ 1.2487,  0.0438, -0.9242, -0.4945,  0.3140,  0.7705],\n        [-1.7318,  0.8625,  0.6469, -1.2476,  0.2633,  0.4192],\n        [ 1.0618, -0.4678, -0.4141,  0.1298, -0.9851, -1.3075],\n        [-1.2418,  0.4950, -1.0060, -0.4789,  1.7964,  0.3920],\n        [-0.0739,  0.5107,  0.9442,  0.4882,  0.7972,  0.1331],\n        [-0.8741, -1.1978,  0.4838,  1.2866, -0.1688,  0.7832],\n        [-0.2664,  0.0565, -1.6116,  0.2955,  1.8965,  0.1646],\n        [ 0.0366,  0.5019, -0.1830, -0.7440,  0.6390,  0.3754],\n        [-1.5394,  0.9430,  0.0688, -1.3277,  0.5918,  0.3106],\n        [-0.8825, -0.1651,  1.7810,  1.4188,  1.4524,  3.6751],\n        [-1.3101,  0.9044, -1.1270,  0.1622,  0.4301,  0.4274],\n        [-1.2291, -0.5768, -0.4269,  0.6353, -0.3544,  1.1477],\n        [-1.8631,  0.2420,  0.1643, -0.6433,  0.2735, -1.6202],\n        [ 0.8742, -0.4123,  0.0243,  0.4301, -0.5812, -1.5734],\n        [ 2.3907, -1.9128, -0.1819,  0.2264, -0.6021, -0.8178],\n        [ 0.1066, -0.2014,  1.2032,  1.1278, -0.8637, -0.6822],\n        [-2.0377, -1.1092, -0.2198, -0.6643, -0.9756, -0.3161],\n        [-2.0832, -1.2231,  0.9117,  1.2083, -0.5331, -3.7652],\n        [ 0.1109,  0.8440,  0.6562,  1.6420, -0.2243, -0.6091],\n        [-0.3966, -1.4157, -1.0054,  1.6644,  1.0481, -2.2824],\n        [ 0.8593,  1.0864, -0.6988, -2.3047, -2.7843, -1.7624],\n        [ 0.0639, -0.9277, -0.3014,  0.0304,  0.5742,  0.8242],\n        [-0.6099, -0.8685, -0.4565, -1.0703, -0.3073,  1.6139],\n        [-0.1290, -0.2425,  0.8798,  0.2450, -2.0916,  0.9882],\n        [ 1.7693,  0.1691,  0.5492,  1.2149,  0.6120, -0.7717],\n        [-0.5200, -1.5747, -0.6167,  1.5255, -0.9525,  1.4009],\n        [ 2.0425,  0.4302,  0.0623,  1.2899,  0.2675, -2.1187],\n        [-0.4913, -0.5743, -0.5416, -0.3563,  0.7916, -3.0967],\n        [-0.7530, -1.5504,  1.0891, -2.1452,  1.1386, -1.5792],\n        [-0.6527,  0.0146,  1.1019, -0.0080, -0.9903, -1.0700],\n        [ 0.6392, -1.0490,  0.4506,  1.1519,  2.5113, -0.9381],\n        [ 0.6494, -0.5501, -0.2913, -0.3182,  0.3390, -1.0882],\n        [-0.5738, -1.1695, -0.0722,  1.3148,  0.5832,  1.2190],\n        [-1.0383,  0.1642, -0.7995,  2.1589,  0.9406, -1.4859],\n        [ 0.8021,  1.1952,  0.2565, -1.0775,  0.5456, -0.6529],\n        [-2.0784, -0.4319,  0.1222,  1.9616,  1.7076,  0.9457],\n        [-0.3679, -1.0891,  3.0273, -0.2574, -1.8382, -0.2704],\n        [ 0.3366,  1.3377, -1.6030, -1.8084,  0.3515,  0.2255],\n        [-1.0384, -0.1892,  0.7247, -1.4856, -1.6840,  1.6841],\n        [ 0.2639, -0.9147, -1.5226,  0.2994, -0.7936,  0.4929],\n        [ 1.2104,  0.5899, -0.4632, -1.4759,  0.5618, -0.3666],\n        [    nan,     nan,     nan,     nan,     nan,     nan],\n        [-0.9922,  1.2268, -1.3738,  0.1937,  0.1562,  1.2054],\n        [ 0.1235,  0.2028, -0.9896,  0.1471, -2.7195, -1.5697],\n        [ 1.9797, -0.2006,  0.0783,  2.0344, -0.1807, -0.9752],\n        [-1.2968, -0.4684, -1.0720, -0.8151,  0.0721,  0.0675],\n        [-0.0721,  0.2649, -1.2464,  0.0762, -0.0620, -0.6290],\n        [ 1.1566, -0.7528,  0.6894, -1.3672,  0.9777, -1.0747],\n        [-1.3331, -0.7280, -0.3100, -0.4092,  1.4556, -1.1141],\n        [-0.7468,  0.3973, -1.3500,  0.7774,  1.7207, -0.1283],\n        [ 0.6562,  1.0001, -1.1725, -1.0226, -1.3935, -0.3621],\n        [ 0.6740,  0.2003,  0.1911,  0.4006, -0.8143, -0.4627],\n        [-0.8300,  0.6091, -0.8033,  1.2451, -0.2504,  0.4903],\n        [ 1.8965, -0.9631,  2.4239, -0.3379, -1.3908, -0.5533],\n        [ 1.2646, -0.0427,  0.2801,  0.5114,  2.2722,  0.2407],\n        [ 0.1550,  1.1291, -0.7998, -0.5746, -0.2857, -0.8539],\n        [ 0.5455, -0.4256,  0.6156, -0.2502, -0.4209,  1.3705],\n        [ 1.5916,  1.5500, -0.0481,  0.3298,  0.1584, -0.2946],\n        [-2.1480,  0.0476, -0.7358, -0.4590,  1.4575, -0.3335],\n        [-1.5940, -0.9643, -1.5169, -1.2065,  0.6945, -0.1607],\n        [-0.6300, -0.0881, -0.6141,  2.2289, -0.1000, -1.3221],\n        [ 1.1030, -0.0307, -1.1783,  0.5076,  0.3657,  0.4713],\n        [ 0.2661,  0.0485, -1.1250, -0.8372,  0.2533,  1.3221],\n        [-2.1721, -1.7865,  0.6894,  0.4613, -0.5411, -1.4074],\n        [-0.1776, -0.8011, -1.1146,  0.4708,  0.8217, -1.2460],\n        [-0.2479,  0.7655, -0.0357,  0.9398, -2.4869,  0.1808],\n        [-1.4515, -1.0143, -1.5391, -0.1395,  0.7175,  1.2454],\n        [-0.9140,  1.2349,  0.1645, -0.3212,  0.9163,  0.6585],\n        [-0.7738, -0.9603, -1.0372, -1.0526, -0.0537,  0.4824],\n        [-0.7669,  2.5333, -0.8542, -0.0389,  0.0495,  0.9145],\n        [-1.7079, -0.7047, -0.5637,  0.1249,  0.4964, -1.8735],\n        [-0.7697,  0.6533, -0.0817,  1.3549,  1.0415,  0.0972],\n        [-1.7306, -0.1716, -1.2382, -0.6274,  1.5101, -0.3711],\n        [ 0.8949,  0.4130,  0.1370, -0.3142,  1.6902,  1.8335],\n        [ 0.4431, -1.7185,  1.0668, -0.1701,  0.2038, -0.5279],\n        [ 1.1476, -1.2762,  0.8320, -1.2620,  1.3044, -0.3816],\n        [-0.9081, -1.5294,  1.1963,  1.0395,  0.2893,  0.2687],\n        [ 0.8759,  0.9352, -1.0875,  0.9397, -0.3736, -0.5558],\n        [ 0.2267,  0.5499,  1.6533, -0.6578,  0.4613,  0.7580],\n        [-1.5385, -1.2783, -0.8521,  0.7387,  1.5360, -1.8494],\n        [-0.7521,  1.1708, -1.6083,  0.8845, -0.7357, -0.7469],\n        [ 0.1079,  1.3595, -0.9786,  0.9924,  0.1227, -2.8517],\n        [-0.3373, -1.2131, -0.9197,  0.7109,  0.9494,  0.7232],\n        [ 0.1262, -0.3787, -0.4148,  0.9990, -0.2726, -0.7937],\n        [-0.5500, -0.4288,  1.5575,  0.7235,  0.3329,  0.6967],\n        [-0.3294,  0.6200,  0.7691, -1.2509,  0.9885,  1.2224],\n        [-2.2292,  1.2639,  0.1065,  0.1434, -0.8692,  0.3048],\n        [-2.4497, -0.2206,  0.0974, -0.1430, -0.8103, -1.0837],\n        [-1.3396,  0.3143, -0.8018, -0.0357,  2.8069, -1.0075],\n        [-1.0969, -0.3382,  0.2405,  0.7578, -0.8351,  0.6986],\n        [ 0.8468,  0.2521, -1.4429,  0.9411, -0.1842, -1.2681],\n        [-0.5355, -0.7613, -0.9023, -0.8214,  0.1331,  1.3853],\n        [-0.1895,  0.5552, -1.5037,  0.4692,  1.0880, -0.1013],\n        [ 1.6791,  1.5355,  0.1618, -0.5281,  1.0045,  1.4819],\n        [ 0.7402, -0.3055,  0.5185, -0.6764,  0.8894,  1.8500]],\n       device='cuda:0', grad_fn=<IndexBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3038453/1015211936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Train and append losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         losses[flowname].append(\n\u001b[0;32m---> 16\u001b[0;31m             train_backward(\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/train.py\u001b[0m in \u001b[0;36mtrain_backward\u001b[0;34m(model, train_data, optimizer, epochs, batch_size, print_n, save_checkpoint, save_best, burn_in, loss_func)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nf.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_forward\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nf.py\u001b[0m in \u001b[0;36mbackward_flow\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/basedistr.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    289\u001b[0m                 \u001b[0;34m\"Expected value argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (128, 6)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([6]), covariance_matrix: torch.Size([6, 6])), but found invalid values:\ntensor([[-0.2675,  1.4909, -0.3712, -0.9898, -0.3368, -0.2115],\n        [ 0.1577,  1.2616, -1.3436, -0.3440,  1.4938,  1.7304],\n        [ 1.2705, -0.7253, -1.3834,  1.0955,  0.1474, -2.7226],\n        [ 0.1669, -1.5974, -0.0104,  0.4517, -1.0284,  0.4189],\n        [ 0.1439,  0.1037,  0.0651, -0.4938,  1.4417,  0.2292],\n        [ 1.2223,  1.2371,  0.0612,  1.1641,  0.8810,  0.2647],\n        [-0.9399, -0.9608,  0.0506, -1.4231,  2.6052,  0.9567],\n        [ 0.7865,  0.8883, -0.2133,  1.3404, -0.0694,  0.3940],\n        [ 0.4920,  1.0669, -1.6345,  0.9416, -0.4395,  0.8280],\n        [-0.9693, -0.7449, -1.9134,  1.1051,  2.3863, -0.4489],\n        [-0.2993, -0.1012, -0.2123,  0.6692, -0.0909,  0.5488],\n        [-1.0780,  2.1041,  0.1545, -0.9265, -0.6986,  0.9396],\n        [ 0.1825, -0.1149,  1.3632,  0.6463, -0.1405, -0.3392],\n        [-0.5964, -1.1183, -0.6785, -1.0986, -0.5350,  0.7550],\n        [-0.3273, -0.0113,  0.9487,  0.7046, -0.1405,  0.8415],\n        [-0.5522, -0.4056, -1.7827,  0.1657,  0.8674, -0.5185],\n        [ 1.0819,  1.0422, -1.7015, -0.4206, -1.7066,  1.1457],\n        [-0.6300, -0.0191, -0.1210, -0.9106, -0.4360,  0.5464],\n        [-1.8629, -0.8358, -0.9142,  0.8180, -0.3310, -1.2395],\n        [ 0.5144,  0.4866,  1.2963,  0.4529,  0.3310,  0.3804],\n        [ 1.0431,  0.1718, -1.2663, -0.2353, -2.8000, -0.9505],\n        [-0.6831, -0.5029, -0.0101,  1.5395, -0.3148, -1.7299],\n        [-1.2932, -0.2658,  0.0957,  1.5588, -0.5864, -0.3406],\n        [-0.1132, -0.7710,  2.1153,  0.4383, -1.0933, -0.6227],\n        [-1.3670, -1.7658, -0.4166,  1.4362,  0.3822,  0.3269],\n        [-0.3028, -0.7639,  1.5479, -0.7053, -1.1363, -1.2451],\n        [-1.3119, -0.6996,  0.6513, -0.7777,  0.3136, -0.5138],\n        [-0.2780, -0.5162,  0.1677, -0.4806,  0.1984,  0.0408],\n        [ 0.0324,  0.2460, -0.6014,  0.2068,  0.5608,  2.9982],\n        [-0.3680,  0.9058,  0.3947, -1.4375, -0.8677,  0.2571],\n        [ 0.6253, -1.0956, -0.8828,  0.7321, -0.2714, -1.2713],\n        [ 0.5201, -1.9279,  1.0629,  0.1377, -0.8069, -1.1453],\n        [ 0.1128,  0.6105,  0.7916, -1.0500, -1.6494, -0.8789],\n        [ 1.2487,  0.0438, -0.9242, -0.4945,  0.3140,  0.7705],\n        [-1.7318,  0.8625,  0.6469, -1.2476,  0.2633,  0.4192],\n        [ 1.0618, -0.4678, -0.4141,  0.1298, -0.9851, -1.3075],\n        [-1.2418,  0.4950, -1.0060, -0.4789,  1.7964,  0.3920],\n        [-0.0739,  0.5107,  0.9442,  0.4882,  0.7972,  0.1331],\n        [-0.8741, -1.1978,  0.4838,  1.2866, -0.1688,  0.7832],\n        [-0.2664,  0.0565, -1.6116,  0.2955,  1.8965,  0.1646],\n        [ 0.0366,  0.5019, -0.1830, -0.7440,  0.6390,  0.3754],\n        [-1.5394,  0.9430,  0.0688, -1.3277,  0.5918,  0.3106],\n        [-0.8825, -0.1651,  1.7810,  1.4188,  1.4524,  3.6751],\n        [-1.3101,  0.9044, -1.1270,  0.1622,  0.4301,  0.4274],\n        [-1.2291, -0.5768, -0.4269,  0.6353, -0.3544,  1.1477],\n        [-1.8631,  0.2420,  0.1643, -0.6433,  0.2735, -1.6202],\n        [ 0.8742, -0.4123,  0.0243,  0.4301, -0.5812, -1.5734],\n        [ 2.3907, -1.9128, -0.1819,  0.2264, -0.6021, -0.8178],\n        [ 0.1066, -0.2014,  1.2032,  1.1278, -0.8637, -0.6822],\n        [-2.0377, -1.1092, -0.2198, -0.6643, -0.9756, -0.3161],\n        [-2.0832, -1.2231,  0.9117,  1.2083, -0.5331, -3.7652],\n        [ 0.1109,  0.8440,  0.6562,  1.6420, -0.2243, -0.6091],\n        [-0.3966, -1.4157, -1.0054,  1.6644,  1.0481, -2.2824],\n        [ 0.8593,  1.0864, -0.6988, -2.3047, -2.7843, -1.7624],\n        [ 0.0639, -0.9277, -0.3014,  0.0304,  0.5742,  0.8242],\n        [-0.6099, -0.8685, -0.4565, -1.0703, -0.3073,  1.6139],\n        [-0.1290, -0.2425,  0.8798,  0.2450, -2.0916,  0.9882],\n        [ 1.7693,  0.1691,  0.5492,  1.2149,  0.6120, -0.7717],\n        [-0.5200, -1.5747, -0.6167,  1.5255, -0.9525,  1.4009],\n        [ 2.0425,  0.4302,  0.0623,  1.2899,  0.2675, -2.1187],\n        [-0.4913, -0.5743, -0.5416, -0.3563,  0.7916, -3.0967],\n        [-0.7530, -1.5504,  1.0891, -2.1452,  1.1386, -1.5792],\n        [-0.6527,  0.0146,  1.1019, -0.0080, -0.9903, -1.0700],\n        [ 0.6392, -1.0490,  0.4506,  1.1519,  2.5113, -0.9381],\n        [ 0.6494, -0.5501, -0.2913, -0.3182,  0.3390, -1.0882],\n        [-0.5738, -1.1695, -0.0722,  1.3148,  0.5832,  1.2190],\n        [-1.0383,  0.1642, -0.7995,  2.1589,  0.9406, -1.4859],\n        [ 0.8021,  1.1952,  0.2565, -1.0775,  0.5456, -0.6529],\n        [-2.0784, -0.4319,  0.1222,  1.9616,  1.7076,  0.9457],\n        [-0.3679, -1.0891,  3.0273, -0.2574, -1.8382, -0.2704],\n        [ 0.3366,  1.3377, -1.6030, -1.8084,  0.3515,  0.2255],\n        [-1.0384, -0.1892,  0.7247, -1.4856, -1.6840,  1.6841],\n        [ 0.2639, -0.9147, -1.5226,  0.2994, -0.7936,  0.4929],\n        [ 1.2104,  0.5899, -0.4632, -1.4759,  0.5618, -0.3666],\n        [    nan,     nan,     nan,     nan,     nan,     nan],\n        [-0.9922,  1.2268, -1.3738,  0.1937,  0.1562,  1.2054],\n        [ 0.1235,  0.2028, -0.9896,  0.1471, -2.7195, -1.5697],\n        [ 1.9797, -0.2006,  0.0783,  2.0344, -0.1807, -0.9752],\n        [-1.2968, -0.4684, -1.0720, -0.8151,  0.0721,  0.0675],\n        [-0.0721,  0.2649, -1.2464,  0.0762, -0.0620, -0.6290],\n        [ 1.1566, -0.7528,  0.6894, -1.3672,  0.9777, -1.0747],\n        [-1.3331, -0.7280, -0.3100, -0.4092,  1.4556, -1.1141],\n        [-0.7468,  0.3973, -1.3500,  0.7774,  1.7207, -0.1283],\n        [ 0.6562,  1.0001, -1.1725, -1.0226, -1.3935, -0.3621],\n        [ 0.6740,  0.2003,  0.1911,  0.4006, -0.8143, -0.4627],\n        [-0.8300,  0.6091, -0.8033,  1.2451, -0.2504,  0.4903],\n        [ 1.8965, -0.9631,  2.4239, -0.3379, -1.3908, -0.5533],\n        [ 1.2646, -0.0427,  0.2801,  0.5114,  2.2722,  0.2407],\n        [ 0.1550,  1.1291, -0.7998, -0.5746, -0.2857, -0.8539],\n        [ 0.5455, -0.4256,  0.6156, -0.2502, -0.4209,  1.3705],\n        [ 1.5916,  1.5500, -0.0481,  0.3298,  0.1584, -0.2946],\n        [-2.1480,  0.0476, -0.7358, -0.4590,  1.4575, -0.3335],\n        [-1.5940, -0.9643, -1.5169, -1.2065,  0.6945, -0.1607],\n        [-0.6300, -0.0881, -0.6141,  2.2289, -0.1000, -1.3221],\n        [ 1.1030, -0.0307, -1.1783,  0.5076,  0.3657,  0.4713],\n        [ 0.2661,  0.0485, -1.1250, -0.8372,  0.2533,  1.3221],\n        [-2.1721, -1.7865,  0.6894,  0.4613, -0.5411, -1.4074],\n        [-0.1776, -0.8011, -1.1146,  0.4708,  0.8217, -1.2460],\n        [-0.2479,  0.7655, -0.0357,  0.9398, -2.4869,  0.1808],\n        [-1.4515, -1.0143, -1.5391, -0.1395,  0.7175,  1.2454],\n        [-0.9140,  1.2349,  0.1645, -0.3212,  0.9163,  0.6585],\n        [-0.7738, -0.9603, -1.0372, -1.0526, -0.0537,  0.4824],\n        [-0.7669,  2.5333, -0.8542, -0.0389,  0.0495,  0.9145],\n        [-1.7079, -0.7047, -0.5637,  0.1249,  0.4964, -1.8735],\n        [-0.7697,  0.6533, -0.0817,  1.3549,  1.0415,  0.0972],\n        [-1.7306, -0.1716, -1.2382, -0.6274,  1.5101, -0.3711],\n        [ 0.8949,  0.4130,  0.1370, -0.3142,  1.6902,  1.8335],\n        [ 0.4431, -1.7185,  1.0668, -0.1701,  0.2038, -0.5279],\n        [ 1.1476, -1.2762,  0.8320, -1.2620,  1.3044, -0.3816],\n        [-0.9081, -1.5294,  1.1963,  1.0395,  0.2893,  0.2687],\n        [ 0.8759,  0.9352, -1.0875,  0.9397, -0.3736, -0.5558],\n        [ 0.2267,  0.5499,  1.6533, -0.6578,  0.4613,  0.7580],\n        [-1.5385, -1.2783, -0.8521,  0.7387,  1.5360, -1.8494],\n        [-0.7521,  1.1708, -1.6083,  0.8845, -0.7357, -0.7469],\n        [ 0.1079,  1.3595, -0.9786,  0.9924,  0.1227, -2.8517],\n        [-0.3373, -1.2131, -0.9197,  0.7109,  0.9494,  0.7232],\n        [ 0.1262, -0.3787, -0.4148,  0.9990, -0.2726, -0.7937],\n        [-0.5500, -0.4288,  1.5575,  0.7235,  0.3329,  0.6967],\n        [-0.3294,  0.6200,  0.7691, -1.2509,  0.9885,  1.2224],\n        [-2.2292,  1.2639,  0.1065,  0.1434, -0.8692,  0.3048],\n        [-2.4497, -0.2206,  0.0974, -0.1430, -0.8103, -1.0837],\n        [-1.3396,  0.3143, -0.8018, -0.0357,  2.8069, -1.0075],\n        [-1.0969, -0.3382,  0.2405,  0.7578, -0.8351,  0.6986],\n        [ 0.8468,  0.2521, -1.4429,  0.9411, -0.1842, -1.2681],\n        [-0.5355, -0.7613, -0.9023, -0.8214,  0.1331,  1.3853],\n        [-0.1895,  0.5552, -1.5037,  0.4692,  1.0880, -0.1013],\n        [ 1.6791,  1.5355,  0.1618, -0.5281,  1.0045,  1.4819],\n        [ 0.7402, -0.3055,  0.5185, -0.6764,  0.8894,  1.8500]],\n       device='cuda:0', grad_fn=<IndexBackward0>)"
     ]
    }
   ],
   "source": [
    "losses = {n: [] for n in flows.keys()}\n",
    "optimizers = {n: [] for n in flows.keys()}\n",
    "\n",
    "for flowname in flows.keys():\n",
    "    for j in range(num_exp):\n",
    "        # Initialize flow\n",
    "        flow = flows[flowname][j]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-4, weight_decay=0)\n",
    "        optimizers[flowname].append(optimizer)\n",
    "        \n",
    "        # Train and append losses\n",
    "        losses[flowname].append(\n",
    "            train_backward(\n",
    "                flow, \n",
    "                dataset.get_training_data(), \n",
    "                optimizer, \n",
    "                epochs, \n",
    "                batch_size, \n",
    "                print_n=200, \n",
    "                save_checkpoint=True, \n",
    "                burn_in=-1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Move flow to CPU\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_flows = {n:[] for n in flows.keys()}\n",
    "for flowname in flows.keys():\n",
    "    for i in range(num_exp):\n",
    "        best_flows[flowname].append(load_best_model(flows[flowname][i]))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3038453/80019021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflowname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflowname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'exp{i}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{flowname}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_scale = True\n",
    "from_iter = 50 * batch_size\n",
    "\n",
    "fig, ax = plt.subplots(2, len(flows), figsize=(12,6))\n",
    "for j, flowname in enumerate(flows.keys()):\n",
    "    for i in range(num_exp):\n",
    "        ax[0,j].plot(losses[flowname][i], label=f'exp{i}', alpha=0.8)\n",
    "        ax[0,j].set_title(f'{flowname}')\n",
    "        if log_scale:\n",
    "            ax[0, j].set_yscale('log')\n",
    "        ax[0, j].legend()\n",
    "\n",
    "log_scale = True\n",
    "for j, flowname in enumerate(flows.keys()):\n",
    "    for i in range(num_exp):\n",
    "        x = np.arange(len(losses[flowname][i]))\n",
    "        ax[1,j].plot(x[from_iter:], losses[flowname][i][from_iter:], label=f'exp{i}', alpha=0.8)\n",
    "        if log_scale:\n",
    "            ax[1, j].set_yscale('log')\n",
    "        ax[1, j].legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results based on training data:\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.13 GiB (GPU 0; 10.76 GiB total capacity; 242.68 MiB already allocated; 1.72 GiB free; 276.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3038453/4224917259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mupdate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_flows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mlog_lik\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_flows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mflow_lh_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur_res\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean loglikelihood for {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{flow} {i}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/scores.py\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(data, model_1, *add_models)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlog_likelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mlog_likelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nf.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mz_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog_det_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/structure/ar.py\u001b[0m in \u001b[0;36mbackward_flow\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermute_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nets/made.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Master/NormalizingFlows/src/nets/made.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMADE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 0; 10.76 GiB total capacity; 242.68 MiB already allocated; 1.72 GiB free; 276.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "flow_lh_results = {\n",
    "    'train': {n:[] for n in flows}, \n",
    "    'test': {n:[] for n in flows}, \n",
    "}\n",
    "cur_res = 'train'\n",
    "\n",
    "print('Results based on training data:' + '\\n')\n",
    "\n",
    "# Function for CI\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    import scipy.stats\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    " \n",
    "\n",
    "train_data = dataset.get_training_data()\n",
    "\n",
    "for flow in best_flows:\n",
    "    for i in range(num_exp):\n",
    "        update_device(device, best_flows[flow][i], dataset)\n",
    "        train_data = dataset.get_training_data()\n",
    "        log_lik, mean = log_likelihood(train_data, best_flows[flow][i])\n",
    "        flow_lh_results[cur_res][flow].append(mean[0])\n",
    "        print(\"Mean loglikelihood for {}: {}\".format(f'{flow} {i}', mean))\n",
    "\n",
    "    \n",
    "    m, h = mean_confidence_interval(flow_lh_results[cur_res][flow])\n",
    "    print('-'*75)\n",
    "    print(f'{flow} -- Mean LogLH Bootstrap: {m:7.4f}  {h:7.4f}')\n",
    "    print('-'*75)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results based on test data\n",
      "\n",
      "Mean loglikelihood for ContinuousPiecewiseAffineAffine exp3 pow 0: [0.15158509]\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3038453/53268525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_flows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mupdate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_flows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Results based on test data' + '\\n')\n",
    "cur_res = 'test' \n",
    "\n",
    "test_data = dataset.get_test_data()\n",
    "\n",
    "for flow in best_flows:\n",
    "    for i in range(num_exp):\n",
    "        update_device(device_cpu, best_flows[flow][i], dataset)\n",
    "        \n",
    "        \n",
    "        test_data = dataset.get_test_data()\n",
    "        log_lik, mean = log_likelihood(test_data, best_flows[flow][i])\n",
    "        flow_lh_results[cur_res][flow].append(mean[0])\n",
    "        print(\"Mean loglikelihood for {}: {}\".format(f'{flow} {i}', mean))\n",
    "\n",
    "        print()\n",
    "    \n",
    "    m, h = mean_confidence_interval(flow_lh_results[cur_res][flow])\n",
    "    print('-'*75)\n",
    "    print(f'{flow} -- Mean LogLH Bootstrap: {m:7.4f}  {h:7.4f}')\n",
    "    print('-'*75)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "# for flow in best_flows:\n",
    "#     log_lik, mean = log_likelihood(test_data, flow)\n",
    "#     print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "#     print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "    \n",
    "\n",
    "# print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
