{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from NormalizingFlows.src.train import train_backward_with_tuning, train_backward\n",
    "from NormalizingFlows.src.scores import log_likelihood, difference_loglik\n",
    "from NormalizingFlows.src.utils import update_device, load_best_model, load_checkpoint_model\n",
    "from NormalizingFlows.src.flows import create_flows\n",
    "\n",
    "from NormalizingFlows.src.structure.ar import AR \n",
    "from NormalizingFlows.src.structure.iar import IAR\n",
    "from NormalizingFlows.src.structure.twoblock import TwoBlock\n",
    "\n",
    "from NormalizingFlows.src.transforms.affine import Affine\n",
    "from NormalizingFlows.src.transforms.piecewise import PiecewiseAffine\n",
    "from NormalizingFlows.src.transforms.piecewise_additive import PiecewiseAffineAdditive\n",
    "from NormalizingFlows.src.transforms.piecewise_affine import PiecewiseAffineAffine\n",
    "\n",
    "from NormalizingFlows.src.data.toydata import ToyDataset\n",
    "from NormalizingFlows.src.data.bsds300 import BSDS300\n",
    "from NormalizingFlows.src.data.gas import Gas\n",
    "from NormalizingFlows.src.data.hepmass import Hepmass\n",
    "from NormalizingFlows.src.data.miniboone import Miniboone\n",
    "from NormalizingFlows.src.data.power import Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1000000 Validation size: 50000 Test size: 250000\n",
      "Dimension: 63\n"
     ]
    }
   ],
   "source": [
    "dataset = BSDS300()\n",
    "#dataset = Gas()\n",
    "#dataset = Hepmass()\n",
    "#dataset = Miniboone()\n",
    "#dataset = Power()\n",
    "\n",
    "print('Training size:', dataset.train_n, 'Validation size:', dataset.valid_n, 'Test size:', dataset.test_n)\n",
    "print('Dimension:', dataset.dim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = dataset.dim_input\n",
    "\n",
    "num_trans = 10\n",
    "perm_type = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_hidden = [126,126,105,105, 20, 10]\n",
    "\n",
    "flows, names = [], []\n",
    "flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "            transformation=PiecewiseAffine)), names.append('PAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "#            transformation=PiecewiseAffineAdditive)), names.append('PAFAd')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "#            transformation=PiecewiseAffineAffine)), names.append('PAFAf')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, permtype, flow_forward=False, structure=AR, \n",
    "#            transformation=Affine)), names.append('MAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, 2*num_trans, permtype, flow_forward=False, structure=AR, \n",
    "#            transformation=Affine)), names.append('MAF-double')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=Affine)), names.append('Real NVP')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, 2*num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=Affine)), names.append('Real NVP-double')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffine)), names.append('TwoBlock-PAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffineAdditive)), names.append('TwoBlock-PAFAd')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffineAffine)), names.append('TwoBlock-PAFAf')\n",
    "\n",
    "for ind, flow in enumerate(flows):\n",
    "    flow.name = names[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning = False\n",
    "if tuning:\n",
    "    losses = []\n",
    "    optimizers = []\n",
    "\n",
    "    epochs = 200\n",
    "    batch_size = 16\n",
    "    num_hyperparam_samples = 4\n",
    "\n",
    "    config = {\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'weight_decay': tune.loguniform(1e-5, 1e-1)\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr='training_iteration',\n",
    "        metric=\"loss\",\n",
    "        mode='min',\n",
    "        max_t=epochs,\n",
    "        grace_period=100,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    reporter=CLIReporter(\n",
    "        metric_columns=['loss', 'training_iteration']\n",
    "    )\n",
    "\n",
    "    for ind, flow in enumerate(flows):\n",
    "        update_device(device_cpu, flow, dataset)\n",
    "        result = tune.run(\n",
    "            partial(train_backward_with_tuning, model=flow, dataset=dataset, epochs=epochs, batch_size=batch_size, print_n=epochs+1, name=names[ind]),\n",
    "            config=config,\n",
    "            num_samples=num_hyperparam_samples,\n",
    "            scheduler=scheduler,\n",
    "            progress_reporter=reporter,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-75cd15d2ee5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3, weight_decay=1e-2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "training = True\n",
    "if training:\n",
    "    losses = []\n",
    "    optimizers = []\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 800\n",
    "\n",
    "    for i in range(len(flows)):\n",
    "        flow = flows[i]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        #optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "        optimizer = torch.optim.SGD(flow.parameters(), lr=1e-4)\n",
    "        optimizers.append(optimizer)\n",
    "\n",
    "        losses.append(train_backward(flow, dataset.get_training_data(), optimizer, epochs, batch_size, print_n=200, save_checkpoint=True, burn_in=-1))\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional training with same optimizer\n",
    "additional_training = False\n",
    "if additional_training:\n",
    "    epochs = 1\n",
    "    batch_size = 16\n",
    "    add_flows, add_optimizers, add_losses = [], [], []\n",
    "    for i in range(len(flows)):\n",
    "        flow, optimizer, loss = load_checkpoint_model(flows[i], optimizers[i])\n",
    "        add_flows.append(flow)\n",
    "        add_optimizers.append(optimizer)\n",
    "        add_losses.append(loss)\n",
    "    \n",
    "    flows, optimizers, losses = add_flows, add_optimizers, add_losses\n",
    "    \n",
    "    for i in range(len(flows)):\n",
    "        flow = flows[i]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        optimizer = optimizers[i]\n",
    "\n",
    "        losses[i] += (train_backward(flow, dataset.get_training_data(), optimizer, epochs, batch_size, print_n=100, save_checkpoint=True, burn_in=1))\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_flows = []\n",
    "for flow in flows:\n",
    "    best_flows.append(load_best_model(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scale = False\n",
    "num_epoch_skip = 0\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(losses)):\n",
    "    plt.plot(losses[i], label=names[i], alpha=0.8)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for i in range(len(losses)):\n",
    "    plt.plot((losses[i])[num_epoch_skip:], label=names[i], alpha=0.8)\n",
    "plt.legend()\n",
    "\n",
    "if log_scale:\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on training data:' + '\\n')\n",
    "\n",
    "train_data = dataset.get_training_data()\n",
    "for flow in best_flows:\n",
    "    log_lik, mean = log_likelihood(train_data, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on validation data' + '\\n')\n",
    "\n",
    "valid_data = dataset.get_validation_data()\n",
    "for flow in best_flows:           \n",
    "    log_lik, mean = log_likelihood(valid_data, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on test data' + '\\n')\n",
    "\n",
    "test_data = dataset.get_test_data()\n",
    "for flow in best_flows:           \n",
    "    log_lik, mean = log_likelihood(test_data, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
