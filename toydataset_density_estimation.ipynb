{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from NormalizingFlows.src.train import train_backward_with_tuning, train_backward\n",
    "from NormalizingFlows.src.scores import log_likelihood, difference_loglik\n",
    "from NormalizingFlows.src.utils import update_device, load_best_model, load_checkpoint_model\n",
    "from NormalizingFlows.src.flows import create_flows\n",
    "\n",
    "from NormalizingFlows.src.structure.ar import AR \n",
    "from NormalizingFlows.src.structure.iar import IAR\n",
    "from NormalizingFlows.src.structure.twoblock import TwoBlock\n",
    "\n",
    "from NormalizingFlows.src.transforms.affine import Affine\n",
    "from NormalizingFlows.src.transforms.piecewise import PiecewiseAffine\n",
    "from NormalizingFlows.src.transforms.piecewise_additive import PiecewiseAffineAdditive\n",
    "from NormalizingFlows.src.transforms.piecewise_affine import PiecewiseAffineAffine\n",
    "\n",
    "from NormalizingFlows.src.data.toydata import ToyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_distr = torch.distributions.exponential.Exponential(torch.tensor([10.0]).to(device))\n",
    "#target_distr = torch.distributions.gamma.Gamma(torch.tensor([1.0]).to(device), torch.tensor([5.0]).to(device))\n",
    "#target_distr = torch.distributions.cauchy.Cauchy(torch.tensor([4.0]), torch.tensor([2.0]))\n",
    "#dataset = ToyDataset(data_distr=target_distr, dim_input=4, samples=1000)\n",
    "\n",
    "dataset = ToyDataset(dim_input=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = dataset.dim_input\n",
    "num_trans = 5\n",
    "perm_type = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_hidden = [126,126,105,105, 20, 10]\n",
    "\n",
    "flows, names = [], []\n",
    "flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "            transformation=PiecewiseAffine)), names.append('PAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "#            transformation=PiecewiseAffineAdditive)), names.append('PAFAd')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=AR, \n",
    "#            transformation=PiecewiseAffineAffine)), names.append('PAFAf')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, permtype, flow_forward=False, structure=AR, \n",
    "#            transformation=Affine)), names.append('MAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, 2*num_trans, permtype, flow_forward=False, structure=AR, \n",
    "#            transformation=Affine)), names.append('MAF-double')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=Affine)), names.append('Real NVP')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, 2*num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=Affine)), names.append('Real NVP-double')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffine)), names.append('TwoBlock-PAF')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffineAdditive)), names.append('TwoBlock-PAFAd')\n",
    "\n",
    "#flows.append(create_flows(dim_input, dim_hidden, num_trans, perm_type, flow_forward=False, structure=TwoBlock,\n",
    "#            transformation=PiecewiseAffineAffine)), names.append('TwoBlock-PAFAf')\n",
    "\n",
    "for ind, flow in enumerate(flows):\n",
    "    flow.name = names[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tuning = False\n",
    "if tuning:\n",
    "    losses = []\n",
    "    optimizers = []\n",
    "\n",
    "    epochs = 200\n",
    "    batch_size = 16\n",
    "    num_hyperparam_samples = 4\n",
    "\n",
    "    config = {\n",
    "        'lr': tune.loguniform(1e-4, 1e-1),\n",
    "        'weight_decay': tune.loguniform(1e-5, 1e-1)\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr='training_iteration',\n",
    "        metric=\"loss\",\n",
    "        mode='min',\n",
    "        max_t=epochs,\n",
    "        grace_period=100,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    reporter=CLIReporter(\n",
    "        metric_columns=['loss', 'training_iteration']\n",
    "    )\n",
    "\n",
    "    for ind, flow in enumerate(flows):\n",
    "        update_device(device_cpu, flow, dataset)\n",
    "        result = tune.run(\n",
    "            partial(train_backward_with_tuning, model=flow, dataset=dataset, epochs=epochs, batch_size=batch_size, print_n=epochs+1, name=names[ind]),\n",
    "            config=config,\n",
    "            num_samples=num_hyperparam_samples,\n",
    "            scheduler=scheduler,\n",
    "            progress_reporter=reporter,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training. Loss for last epoch PAF:      2.32512\n"
     ]
    }
   ],
   "source": [
    "training = True\n",
    "if training:\n",
    "    losses = []\n",
    "    optimizers = []\n",
    "\n",
    "    epochs = 50\n",
    "    batch_size = 16\n",
    "\n",
    "    for i in range(len(flows)):\n",
    "        flow = flows[i]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        #optimizer = torch.optim.AdamW(flow.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "        optimizer = torch.optim.SGD(flow.parameters(), lr=1e-4)\n",
    "        optimizers.append(optimizer)\n",
    "\n",
    "        losses.append(train_backward(flow, dataset.get_training_data(), optimizer, epochs, batch_size, print_n=200, save_checkpoint=True, burn_in=-1))\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional training with same optimizer\n",
    "additional_training = False\n",
    "if additional_training:\n",
    "    epochs = 200\n",
    "    batch_size = 16\n",
    "    add_flows, add_optimizers, add_losses = [], [], []\n",
    "    for i in range(len(flows)):\n",
    "        flow, optimizer, loss = load_checkpoint_model(flows[i], optimizers[i])\n",
    "        add_flows.append(flow)\n",
    "        add_optimizers.append(optimizer)\n",
    "        add_losses.append(loss)\n",
    "    \n",
    "    flows, optimizers, losses = add_flows, add_optimizers, add_losses\n",
    "    \n",
    "    for i in range(len(flows)):\n",
    "        flow = flows[i]\n",
    "        update_device(device, flow, dataset)\n",
    "\n",
    "        optimizer = optimizers[i]\n",
    "\n",
    "        losses[i] += (train_backward(flow, dataset.get_training_data(), optimizer, epochs, batch_size, print_n=100, save_checkpoint=True, burn_in=1))\n",
    "\n",
    "        update_device(device_cpu, flow, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_flows = []\n",
    "for flow in flows:\n",
    "    best_flows.append(load_best_model(flow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scale = False\n",
    "from_iter = 75\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(losses)):\n",
    "    plt.plot(losses[i], label=names[i], alpha=0.8)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for i in range(len(losses)):\n",
    "    plt.plot((losses[i])[from_iter:], label=names[i], alpha=0.8)\n",
    "plt.legend()\n",
    "\n",
    "if log_scale:\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on training data:' + '\\n')\n",
    "\n",
    "train_data = dataset.get_training_data()\n",
    "mean_target = torch.mean(dataset.evaluate(train_data)).detach().numpy()\n",
    "for flow in best_flows:\n",
    "    log_lik, mean = log_likelihood(train_data, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "    print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "    \n",
    "    log_lik_diff, mean_diff, median_diff = difference_loglik(train_data, dataset, flow)\n",
    "    print(\"Mean difference from target loglikelihood for {}: {}\".format(str(flow), mean_diff))\n",
    "    print(\"Median difference from target loglikelihood for {}: {} \\n\".format(str(flow), median_diff))\n",
    "    \n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on test data' + '\\n')\n",
    "\n",
    "test_data = dataset.get_test_data()\n",
    "mean_target = torch.mean(dataset.evaluate(test_data)).detach().numpy()\n",
    "for flow in best_flows:\n",
    "    log_lik, mean = log_likelihood(test_data, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "    print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "    \n",
    "    log_lik_diff, mean_diff, median_diff = difference_loglik(test_data, dataset, flow)\n",
    "    print(\"Mean difference from target loglikelihood for {}: {}\".format(str(flow), mean_diff))\n",
    "    print(\"Median difference from target loglikelihood for {}: {} \\n\".format(str(flow), median_diff))\n",
    "    \n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results based on new sample from each flow:' + '\\n')\n",
    "\n",
    "mean_target = torch.mean(dataset.evaluate(sample_last)).detach().numpy()\n",
    "for flow in best_flows:\n",
    "    with torch.no_grad():\n",
    "        sample, log_prob = flow.sample(800)\n",
    "        sample_last = sample[-1]\n",
    "        \n",
    "    log_lik, mean = log_likelihood(sample_last, flow)\n",
    "    print(\"Mean loglikelihood for {}: {}\".format(str(flow), mean))\n",
    "    print(\"Difference between target and {} mean loglikelihood: {}\".format(str(flow), abs(mean-mean_target)))\n",
    "    \n",
    "    log_lik_diff, mean_diff, median_diff = difference_loglik(sample_last, dataset, flow)\n",
    "    print(\"Mean difference from target loglikelihood for {}: {}\".format(str(flow), mean_diff))\n",
    "    print(\"Median difference from target loglikelihood for {}: {} \\n\".format(str(flow), median_diff))\n",
    "\n",
    "\n",
    "print(\"Mean loglikelihood with actual distribution: {}\".format(mean_target))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
